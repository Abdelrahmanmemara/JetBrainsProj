{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Corrector\n",
    "\n",
    "- We use Birbeck dataset, as it gives us a large and versatile combination of data to test the chosen models.\n",
    "\n",
    "- We will work with two pre-trained models, and analyze and compare their performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1:- CHOOSING AND MODIFYING THE DATASET\n",
    "\n",
    "- We choose the Birbeck dataset due to its large and diverse combination of words. \n",
    "\n",
    "- I downloaded it and converted into a CSV File.\n",
    "\n",
    "- I need to make some changes to the CSV File to make the testing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/wrong_words.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The correct word is the word that has a dollar sign at the start.\n",
    "\n",
    "- So the idea is that we want to make another column, that has the correct word for each misspelled word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$Albert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ameraca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amercia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word\n",
       "0   $Albert\n",
       "1        Ab\n",
       "2  $America\n",
       "3   Ameraca\n",
       "4   Amercia"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$Albert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ameraca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amercia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ameracan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$April</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Apirl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word\n",
       "0    $Albert\n",
       "1         Ab\n",
       "2   $America\n",
       "3    Ameraca\n",
       "4    Amercia\n",
       "5  $American\n",
       "6   Ameracan\n",
       "7     $April\n",
       "8      Apirl"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_data = data[0:9]\n",
    "\n",
    "subset_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$manage'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Word\"][23551]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_words = []\n",
    "i = 0\n",
    "j = 1\n",
    "\n",
    "while j < data.shape[0] and i < data.shape[0]:\n",
    "\n",
    "    word1 = data[\"Word\"][i]\n",
    "    word2 = data[\"Word\"][j]\n",
    "    if \"$\" in word1:\n",
    "        if \"$\" in word2:\n",
    "            correct_word = word1.replace(\"$\", \"\")\n",
    "            correct_words += [correct_word for _ in range(j-i)]\n",
    "            i = j\n",
    "            j += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "\n",
    "if i < data.shape[0]:\n",
    "    word1 = data[\"Word\"][i+1]\n",
    "    correct_word = word1.replace(\"$\", \"\")\n",
    "    correct_words += [correct_word for _ in range(j-i)]\n",
    "    \n",
    "data[\"Correct_word\"] = correct_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Correct_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$Albert</td>\n",
       "      <td>Albert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab</td>\n",
       "      <td>Albert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$America</td>\n",
       "      <td>America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ameraca</td>\n",
       "      <td>America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amercia</td>\n",
       "      <td>America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Correct_word\n",
       "0   $Albert       Albert\n",
       "1        Ab       Albert\n",
       "2  $America      America\n",
       "3   Ameraca      America\n",
       "4   Amercia      America"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Will remove the rows that has the correct words, as there no need to have their own row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~data[\"Word\"].str.contains(\"\\$\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Correct_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ab</td>\n",
       "      <td>Albert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ameraca</td>\n",
       "      <td>America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amercia</td>\n",
       "      <td>America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ameracan</td>\n",
       "      <td>American</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Apirl</td>\n",
       "      <td>April</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word Correct_word\n",
       "1        Ab       Albert\n",
       "3   Ameraca      America\n",
       "4   Amercia      America\n",
       "6  Ameracan     American\n",
       "8     Apirl        April"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2:- CHOOSING THE METRICS \n",
    "\n",
    "The chosen Metrics:-\n",
    " \n",
    "1) Accuracy: Accuracy is a straightforward and intuitive metric that directly reflects how often the spell checker provides the correct correction. It’s useful as an overall performance indicator, providing a clear picture of the tool’s effectiveness. I chose Accuracy because it offers a simple yet comprehensive assessment of the spell checker’s ability to make the right corrections without needing to analyze the rank or multiple options.\n",
    "\n",
    "2) Mean Reciprocal Rank (MRR): MRR is particularly beneficial for spell checkers that generate multiple suggestions, as it rewards tools that place the correct answer higher on the list. This metric is ideal for applications where the user can choose from several suggested corrections. I chose MRR because it captures not only if the correct answer is present but also how well-ranked it is, adding a layer of quality assessment to the tool’s suggestions.\n",
    "\n",
    "Reasons for Choosing These Over Other Metrics:\n",
    "\n",
    "- Precision & Recall: While these metrics are useful for evaluating over-correction and under-correction, they are more suited for applications where false positives and false negatives carry different weights, such as in classification tasks. For spell checking, Accuracy and MRR provide a more holistic view of performance.\n",
    "- Edit Distance: This metric measures the number of changes needed to transform the misspelled word into the correct word. However, it may not fully capture the quality of the spell checker’s suggestions, especially when multiple suggestions are offered. MRR, in contrast, focuses on the rank of correct suggestions, which is more relevant for assessing user-facing spell checkers.\n",
    "These choices balance the need for an overall accuracy assessment (Accuracy) with a focus on ranking quality (MRR), making them well-suited to evaluating spell checkers in a practical, user-oriented context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3:- CHOOSING THE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdel\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# Load a grammar and spell correction model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prithivida/grammar_error_correcter_v1\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"prithivida/grammar_error_correcter_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_data = data.iloc[:3000,:]\n",
    "\n",
    "words = subset_data[\"Word\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdel\\AppData\\Local\\Temp\\ipykernel_15724\\2003504245.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  subset_data[\"BERT_word\"] = sentences\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "for sentence in words:\n",
    "    # Encode and generate correction\n",
    "    inputs = tokenizer.encode(\"gec: \" + sentence, return_tensors=\"pt\")\n",
    "    outputs = model.generate(inputs, max_length=128, num_beams=5, early_stopping=True)\n",
    "    corrected_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    sentences.append(corrected_sentence)\n",
    "\n",
    "subset_data[\"BERT_word\"] = sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load T5 model for spell correction\n",
    "tokenizer_T5 = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model_T5 = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for sentence in words:\n",
    "    # Encode and generate correction\n",
    "    inputs = tokenizer_T5.encode(\"gec: \" + sentence, return_tensors=\"pt\")\n",
    "    outputs = model_T5.generate(inputs, max_length=128, num_beams=5, early_stopping=True)\n",
    "    corrected_sentence = tokenizer_T5.decode(outputs[0], skip_special_tokens=True)\n",
    "    sentences.append(corrected_sentence)\n",
    "\n",
    "subset_data[\"T5_word\"] = sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4:- Evaluating the models \n",
    "\n",
    "- We first need to create functions that calculate the Accuracy and the MRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predictions, ground_truth):\n",
    "    correct_count = 0\n",
    "    total = len(ground_truth)\n",
    "    for i, correct_word in enumerate(ground_truth):\n",
    "        # Check if the correct word is in the top prediction (first suggestion)\n",
    "        if predictions.iloc[i] == correct_word:\n",
    "            correct_count += 1\n",
    "    \n",
    "    accuracy = correct_count / total\n",
    "    return accuracy\n",
    "\n",
    "def calculate_mrr(predictions, ground_truth):\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for i, correct_word in enumerate(ground_truth):\n",
    "        if correct_word in predictions.iloc[i]:\n",
    "            rank = predictions.iloc[i].index(correct_word) + 1  # +1 because ranks are 1-based\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0)  # No correct suggestion in the list\n",
    "    \n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The accuracy of the **BERT Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_BERT = subset_data[\"BERT_word\"]\n",
    "ground_truth = subset_data[\"Word\"]\n",
    "\n",
    "accuracy_BERT = calculate_accuracy(predictions_BERT, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_BERT = calculate_mrr(predictions_BERT, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for BERT: 0.25\n",
      "MRR for BERT: 0.81\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for BERT: {accuracy_BERT:.2f}\")\n",
    "print(f\"MRR for BERT: {mrr_BERT:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Accuracy of the T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_T5 = subset_data[\"T5_word\"]\n",
    "\n",
    "accuracy_T5 = calculate_accuracy(predictions_T5, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrr_T5 = calculate_mrr(predictions_T5, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for T5: 0.20\n",
      "MRR for T5: 0.32\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for T5: {accuracy_T5:.2f}\")\n",
    "print(f\"MRR for T5: {mrr_T5:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 \n",
    "\n",
    "1) Strengths and Weaknesses\n",
    "\n",
    "    - BERT Model:\n",
    "\n",
    "- Accuracy: The BERT model achieved an accuracy of 0.25, meaning it correctly identified the exact spelling for 25% of the cases. This indicates that while BERT is somewhat effective in suggesting the correct spelling, its primary strength lies not in providing a high number of exact matches but in its contextual understanding.\n",
    "- MRR: The Mean Reciprocal Rank for BERT is 0.81, suggesting that even when BERT doesn’t provide the correct word as the top suggestion, it often ranks it highly among its suggestions. This high MRR value highlights BERT's ability to produce relevant corrections that may not be perfect but are close to the correct answer. The strong MRR score implies that BERT’s suggestions could be useful if further optimized or if the tool allows users to select from multiple suggestions.\n",
    "- Weakness: Although BERT performs well in ranking relevant suggestions, its relatively low accuracy indicates that it often fails to provide the exact correct word as the first suggestion.\n",
    "\n",
    " \t- T5 Model:\n",
    "\n",
    "- Accuracy: The T5 model achieved a lower accuracy of 0.20, meaning it correctly identified the exact spelling for only 20% of the cases. This lower accuracy compared to BERT suggests that T5 might not be as effective in making precise corrections.\n",
    "- MRR: The MRR for T5 is 0.32, which is considerably lower than that of BERT. This indicates that T5’s suggestions are generally less relevant or useful compared to BERT’s, as the correct answer is less likely to be ranked highly among its suggestions.\n",
    "- Weakness: T5’s lower accuracy and MRR suggest that it may lack both the precision and the ranking quality seen in BERT, making it less reliable for tasks requiring top-ranked correct suggestions.\n",
    "\n",
    "2) Improvement Suggestions\n",
    "\n",
    "- Model Ensembling: Given that BERT has a high MRR but moderate accuracy, combining it with other models (e.g., T5 or rule-based spell checkers) might improve the overall accuracy by leveraging the strengths of multiple models. An ensemble approach could enhance the chances of obtaining the correct answer in the top suggestions.\n",
    "- Fine-Tuning: Fine-tuning the BERT model on a specialized spelling correction dataset could help increase its accuracy for exact matches. Tailoring the model to correct specific types of spelling errors might address its shortcomings in providing precise corrections.\n",
    "- Context-Based Re-Ranking: Since BERT provides relatively high-quality suggestions, implementing a context-aware re-ranking mechanism could improve the rank of the correct answer. By taking into account additional linguistic features or common spelling patterns, this approach could optimize the order of BERT’s suggestions for better usability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
